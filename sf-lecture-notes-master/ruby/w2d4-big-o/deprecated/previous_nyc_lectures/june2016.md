## Motivation

Whenever we're starting to solve a problem/ write and algorithm, we usually don't worry about it's efficiency. It's more important that the implementation is easy to understand and maintain. We also don't want to waste time optimizing something that will never cause a problem.

So why do we care about Big O? When dealing with huge datasets, how long our function takes to run can actually stop us from being able to solve the problem AT ALL. Imagine we want to use facebooks database of users to find out how likely it is that any one person, is friends with another. Our initial strategy may be to compare every user, to every other user and see if they are friends. But this requires a billion checks, per user, and there's a billion users! That means we're doing a billion times a billion operations. We probably won't live to see the answer. At this point, we need a way of measure how this function is going to behave as the input size becomes arbitrarily large. Big O notation is a way of formalizing how well an algorithm performs on huge inputs.

## The Cat Psychologist

Let's look at an extremely realistic scenario. We're making a dating app for Cats, and we need to match cats based on their personalities. We have a Cat Psychologist who when given a cat, takes 1 second and then gives them a personality score (not bad!). Our cat matching algorithm is really fast once we have their scores, it takes 0.1 seconds to find their match score when comparing two cats. Let's consider how this plays out with 5 cats.

First, our cat psychologist will give a score to each cat. This will take them 5 seconds, one for each cat. Our comparisons only take 0.1 second, but since we're comparing every cat to every other cat, it's 5 * 5. That's 25 comparisons, so 25 * 0.1. 2 and half seconds. Awesome! It would seem at this point that our cat psychologist is slacking and we should speed him up. He took four times longer than our matchmaking algorithm. Before taking drastic measures, let's try this again with 10 cats.

Our cat psychologist will give a score to each cat. This will take them 10 seconds now. Now we're comparing every cat to every cat again, but this time it's 10 * 10 (100). 100 * 0.1, is ten, so even at 10 cats, our comparisons are now taking the same amount of time as the scoring. You can guess what happens with an input size of 1000, now we're doing 1000 * 1000 comparisons. That's one million times 0.1, or a hundred thousand seconds. Clearly our cat psychologist was not the bottleneck.

So how could we have figured this out using Big O notation? Well we know that the cat psychologist does 10 operations for 10 cats, 100 for 100 etc etc, so if the input size is N, the psychologist does N operations. That portion of the algorithm runs in O(n), the number of operations is directly proportional, to the input size.

How about the comparisons? Well for N cats, we do N * N operations. That means the comparisons are O(n^2). So our whole time complexity is O(n + n^2).

As we can see through our concrete examples, as the input size grows arbitrarily large, the n becomes totally negligeble. So much so that we don't even consider it part of the time complexity. We would say that O(n^2) portion of the algorithm eclipses the O(n) portion. In general, you only care about the part of your algorithm with THE WORST time complexity, since as the input size gets huge, that it the portion that will determine how long it runs.


## Examples of other time complexities

### Binary Search O(log n)

We've already seen O(n) and O(n^2). Let's take a look at one of the best possible time complexties, O(log n). A classic example of this is our old friend Binary Search. So why is it log n?

Consider how many operations it takes to run a binary search, first we take the middle element and compare it to the element we're searching for. That's a single operation, but if we don't find it, we perform that same operation on half of the input. That means we run the comparison as many times as we can split the array. Here's an example.


[1, 2, 3, 4, 5, 6, 7, 8]
Here we have eight numbers, we check the middle element and then split it in half (doesn't matter which way)

[1, 2, 3, 4] split 1
Now we're checking four numbers, assuming we can't find it, we split again

[1, 2] split 2
Still haven't found it?

[1] split 3
At this point, it's here or it's not. How many times did we split? 3. What's log2 of 8? It's whatever power we have to raise 2 to, to get 8. 2 * 2 * 2 === 8, so log 2 of 8 is 3!

What if we have input size sixteen?
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]
[1, 2, 3, 4, 5, 6, 7, 8] split 4
[1, 2, 3, 4] split 2
[1, 2] split 3
[1] split 3

We just doubled the input size, and only had to do a single additional operation. That's almost as good as it gets! Just to prove this is still the log, what's log2 of 16? 2 * 2 * 2 * 2 === 6, so we have to raise 2 to the 4th power to get 16, therefore the log2 of 16, if 4.

So why do we say O(log n), and not O(log2 n)? While log2 of n is what is actually happening, in big O notation we don't care about a constant offset. For example O(n^2 /2) is still O(n^2) because it still grows exponentially). By the same token, changing the log base does change the speed of the algorithm, but it doesn't change the way it grows. It's also worth noting that in computer science, we're almost always talking about the natural log or log base 2.

### Merge Sort O(n log n)

The best sorting algorithms in the world operate in n log n time. Merge sort is among them, but why is it n log n? Let's look at the operations we do in merge sort.

Before talking about merge sort itself, it's worth noting that merging two sorted arrays, is O(n), we run an operation on each element once. So since merging is O(n), how many times to we merge? That depends how many times we split! Let's take a look

[6, 4, 1, 3, 2, 8, 5, 7]
[6, 4, 1, 3] [2, 8, 5, 7]
[6, 4] [1, 3] [2, 8] [5, 7]
[6] [4] [1] [3] [2] [8] [5] [7]
Time to merge!
[4, 6] [1, 3] [2, 8] [5, 7] once (n operations)
[1, 3, 4, 6] [2, 5, 7, 8] twice (n operations)
[1, 2, 3, 4] [5, 6, 7, 8] three times (n operations)

that was n times 3. What is the in relation to 8? Three is log2 of 8. Therefore this algorithm runs in O(n log2 n) or just O(n log n).

### Naive Nth fibonacci number O(2^n)

One of the worst (but not the worst) time complexities is 2^n, this means for every additional n, our number of operations double. Yikes! Let's look at some code before we get started.

```ruby
  def fib(n)
    return n if n <= 1
    return fib(n - 1) + fib(n - 2)
  end
```

So how many operations does this do? Well since each call we initiate two other calls, we double the calls to fib each time we add one to n. We could potentially avoid this by making a single recursive call and storing the value, although we would have to formulate our function differently. To help visualize the recursive calls, if we call fib(8), we're looking for 13. How does that get calculated?

0, 1, 1, 2, 3, 5, 8, (13)
                     5 - 8
                 3 - 2    5 - 3
               1-2  1-1  3-2  2-1
           etc etc etc    etc etc etc



### Traveling Salesman O(n!)

Pretty much the worst possible time complexity you can get, factorial time becomes unusable even on relatively small input sizes. Consider a salesman who has N towns to visit, they don't care which order they visit them in, so how many different routes could they take?

With one town, this is simple. One route.
A

With two towns, we have two routes (you can think of it as adding town B to every position in the original array of one, before and after.)
`B` > A
A > `B`

With three towns, we have 6 routes (we add C at every location in each of the previous two routes)
`C` > A > B
A > `C` > B
A > B > `C`

`C` > B > A
B > `C` > A
B > A > `C`

With four towns, we have 24 routes. We're basically injecting the fourth town into every possible position within every previous route. (there are 4 positions, and 6 (or 3!) previous routes...)
`D` > A > B > C
A > `D` > B > C
A > B > `D` > C
A > B > C > `D`

`D` > A > C > B
A > `D` > C > B
A > C > `D` > B
A > C > B > `D`

`D` > B > A > C
B > `D` > A > C
B > A > `D` > C
B > A > C > `D`

`D` > B > C > A
B > `D` > C > A
B > C > `D` > A
B > C > A > `D`

`D` > C > B > A
C > `D` > B > A
C > B > `D` > A
C > B > A > `D`

`D` > C > A > B
C > `D` > A > B
C > A > `D` > B
C > A > B > `D`
